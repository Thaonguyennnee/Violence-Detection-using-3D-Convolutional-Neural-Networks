{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from models.experimental import attempt_load\n",
    "import cv2\n",
    "import torch\n",
    "def load_models():\n",
    "    model = attempt_load(r'yolov7.pt')\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[30], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m device \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 2\u001B[0m image_tensor \u001B[38;5;241m=\u001B[39m \u001B[43mimage_tensor\u001B[49m\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m      3\u001B[0m model \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# Run the image through the model\u001B[39;00m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'image_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the pipeline function\n",
    "\n",
    "def yolov5_pipeline(image_path, model):\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    image_tensor = image_path\n",
    "    # Load the input image and the YOLOv5 model\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    image_tensor = image_tensor.to(device)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "\n",
    "    # Run the image through the model\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        detections = model(image_tensor.to(device))\n",
    "\n",
    "    detections = non_max_suppression(detections,\n",
    "\n",
    "                                     conf_thres=conf,\n",
    "\n",
    "                                     iou_thres=iou)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-09T07:44:17.583026400Z",
     "start_time": "2023-10-09T07:44:17.554257800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1523302932.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  Cell \u001B[1;32mIn[23], line 1\u001B[1;36m\u001B[0m\n\u001B[1;33m    pip install --upgrade ultralytics\u001B[0m\n\u001B[1;37m        ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "    # Load and preprocess the image\n",
    "\n",
    "    image_tensor = preprocess_img(image_path)\n",
    "\n",
    "\n",
    "\n",
    "    # Load the input image and the YOLOv5 model\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    image_tensor = image_tensor.to(device)\n",
    "\n",
    "    model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-09T06:52:33.178033700Z",
     "start_time": "2023-10-09T06:52:33.130786Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Image Not Found C:\\Users\\LAPTOP\\OneDrive\\Tài liệu\\2023\\FALL\\OJT\\ThaoNguyen\\video-recognition\\yolov7\\Screenshot 2023-10-09 132316.png",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 35\u001B[0m\n\u001B[0;32m     32\u001B[0m colors \u001B[38;5;241m=\u001B[39m [[\u001B[38;5;241m255\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m], [\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m255\u001B[39m, \u001B[38;5;241m0\u001B[39m], [\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m255\u001B[39m]]  \u001B[38;5;66;03m# Define colors for classes (you can customize this)\u001B[39;00m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;66;03m# Run inference\u001B[39;00m\n\u001B[1;32m---> 35\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m path, img, im0s, _ \u001B[38;5;129;01min\u001B[39;00m dataset:\n\u001B[0;32m     36\u001B[0m     img \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfrom_numpy(img)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     37\u001B[0m     img \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m255.0\u001B[39m  \u001B[38;5;66;03m# Normalize to [0, 1]\u001B[39;00m\n",
      "File \u001B[1;32m~\\OneDrive\\Tài liệu\\2023\\FALL\\OJT\\ThaoNguyen\\video-recognition\\yolov7\\utils\\datasets.py:187\u001B[0m, in \u001B[0;36mLoadImages.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    185\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcount \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    186\u001B[0m     img0 \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mimread(path)  \u001B[38;5;66;03m# BGR\u001B[39;00m\n\u001B[1;32m--> 187\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m img0 \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mImage Not Found \u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m path\n\u001B[0;32m    188\u001B[0m     \u001B[38;5;66;03m#print(f'image {self.count}/{self.nf} {path}: ', end='')\u001B[39;00m\n\u001B[0;32m    189\u001B[0m \n\u001B[0;32m    190\u001B[0m \u001B[38;5;66;03m# Padded resize\u001B[39;00m\n\u001B[0;32m    191\u001B[0m img \u001B[38;5;241m=\u001B[39m letterbox(img0, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimg_size, stride\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride)[\u001B[38;5;241m0\u001B[39m]\n",
      "\u001B[1;31mAssertionError\u001B[0m: Image Not Found C:\\Users\\LAPTOP\\OneDrive\\Tài liệu\\2023\\FALL\\OJT\\ThaoNguyen\\video-recognition\\yolov7\\Screenshot 2023-10-09 132316.png"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from models.experimental import attempt_load\n",
    "from utils.datasets import LoadImages\n",
    "from utils.general import non_max_suppression, scale_coords, xyxy2xywh, increment_path\n",
    "from utils.plots import plot_one_box\n",
    "\n",
    "# Set your specific settings here\n",
    "weights_path = 'yolov7.pt'  # Path to the YOLOv7 weights file\n",
    "# image_path = 'path_to_your_image.jpg'  # Path to the input image\n",
    "img_size = 640  # Inference size (pixels)\n",
    "conf_thres = 0.25  # Object confidence threshold\n",
    "iou_thres = 0.45  # IOU threshold for NMS\n",
    "\n",
    "# Create save directory\n",
    "save_dir = Path(increment_path(Path('runs/detect'), exist_ok=True))\n",
    "\n",
    "# Initialize\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load model\n",
    "model = attempt_load(weights_path, map_location=device)\n",
    "stride = int(model.stride.max())\n",
    "imgsz = img_size\n",
    "\n",
    "# Set Dataloader\n",
    "dataset = LoadImages(image_path, img_size=imgsz, stride=stride)\n",
    "\n",
    "# Get names and colors\n",
    "names = model.names\n",
    "colors = [[255, 0, 0], [0, 255, 0], [0, 0, 255]]  # Define colors for classes (you can customize this)\n",
    "\n",
    "# Run inference\n",
    "for path, img, im0s, _ in dataset:\n",
    "    img = torch.from_numpy(img).to(device)\n",
    "    img /= 255.0  # Normalize to [0, 1]\n",
    "    if img.ndimension() == 3:\n",
    "        img = img.unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model(img)[0]\n",
    "        pred = non_max_suppression(pred, conf_thres, iou_thres, classes=None, agnostic=False)\n",
    "\n",
    "    # Process detections\n",
    "    for det in pred:\n",
    "        if det is not None and len(det):\n",
    "            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0s.shape).round()\n",
    "            for *xyxy, conf, cls in reversed(det):\n",
    "                label = f'{names[int(cls)]} {conf:.2f}'\n",
    "                plot_one_box(xyxy, im0s, label=label, color=colors[int(cls)], line_thickness=1)\n",
    "\n",
    "    # Save the image with detections\n",
    "    save_path = save_dir / Path(path).name\n",
    "    cv2.imwrite(str(save_path), im0s)\n",
    "\n",
    "print(f'Detections saved to {save_dir}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-09T06:43:57.767154400Z",
     "start_time": "2023-10-09T06:43:56.705227900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (unsigned char) and bias type (float) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 9\u001B[0m\n\u001B[0;32m      6\u001B[0m image_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mScreenshot 2023-10-09 132316.png\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# Perform object detection\u001B[39;00m\n\u001B[1;32m----> 9\u001B[0m \u001B[43mdetect_objects\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[11], line 8\u001B[0m, in \u001B[0;36mdetect_objects\u001B[1;34m(image_path, model)\u001B[0m\n\u001B[0;32m      5\u001B[0m img_tensor \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfrom_numpy(img)\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# Perform inference on the image\u001B[39;00m\n\u001B[1;32m----> 8\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg_tensor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# Process the detection results\u001B[39;00m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# You can access bounding boxes, class IDs, and confidences from results.pred\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;66;03m# For example:\u001B[39;00m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m det \u001B[38;5;129;01min\u001B[39;00m results\u001B[38;5;241m.\u001B[39mpred[\u001B[38;5;241m0\u001B[39m]:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\OJT\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\OneDrive\\Tài liệu\\2023\\FALL\\OJT\\ThaoNguyen\\video-recognition\\yolov7\\models\\yolo.py:599\u001B[0m, in \u001B[0;36mModel.forward\u001B[1;34m(self, x, augment, profile)\u001B[0m\n\u001B[0;32m    597\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcat(y, \u001B[38;5;241m1\u001B[39m), \u001B[38;5;28;01mNone\u001B[39;00m  \u001B[38;5;66;03m# augmented inference, train\u001B[39;00m\n\u001B[0;32m    598\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 599\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward_once\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprofile\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive\\Tài liệu\\2023\\FALL\\OJT\\ThaoNguyen\\video-recognition\\yolov7\\models\\yolo.py:625\u001B[0m, in \u001B[0;36mModel.forward_once\u001B[1;34m(self, x, profile)\u001B[0m\n\u001B[0;32m    622\u001B[0m         dt\u001B[38;5;241m.\u001B[39mappend((time_synchronized() \u001B[38;5;241m-\u001B[39m t) \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m100\u001B[39m)\n\u001B[0;32m    623\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%10.1f\u001B[39;00m\u001B[38;5;132;01m%10.0f\u001B[39;00m\u001B[38;5;132;01m%10.1f\u001B[39;00m\u001B[38;5;124mms \u001B[39m\u001B[38;5;132;01m%-40s\u001B[39;00m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m%\u001B[39m (o, m\u001B[38;5;241m.\u001B[39mnp, dt[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m], m\u001B[38;5;241m.\u001B[39mtype))\n\u001B[1;32m--> 625\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[43mm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# run\u001B[39;00m\n\u001B[0;32m    627\u001B[0m     y\u001B[38;5;241m.\u001B[39mappend(x \u001B[38;5;28;01mif\u001B[39;00m m\u001B[38;5;241m.\u001B[39mi \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msave \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m)  \u001B[38;5;66;03m# save output\u001B[39;00m\n\u001B[0;32m    629\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m profile:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\OJT\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\OneDrive\\Tài liệu\\2023\\FALL\\OJT\\ThaoNguyen\\video-recognition\\yolov7\\models\\common.py:111\u001B[0m, in \u001B[0;36mConv.fuseforward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    110\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfuseforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m--> 111\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mact(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\OJT\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\OJT\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    462\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 463\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\OJT\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[1;34m(self, input, weight, bias)\u001B[0m\n\u001B[0;32m    455\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    456\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[0;32m    457\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[0;32m    458\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[1;32m--> 459\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    460\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Input type (unsigned char) and bias type (float) should be the same"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load the YOLOv7 model\n",
    "    model = load_model()\n",
    "\n",
    "    # Path to the image you want to perform object detection on\n",
    "    image_path = 'Screenshot 2023-10-09 132316.png'\n",
    "\n",
    "    # Perform object detection\n",
    "    detect_objects(image_path, model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-09T06:29:44.086058500Z",
     "start_time": "2023-10-09T06:29:43.008540500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\LAPTOP\\OneDrive\\Tai liu\\2023\\FALL\\OJT\\ThaoNguyen\\video-recognition\\yolov7\\Screenshot 2023-10-09 132316.png: 416x640 2 persons, 180.2ms\n",
      "Speed: 7.1ms preprocess, 180.2ms inference, 31.5ms postprocess per image at shape (1, 3, 416, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": "-1"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "model=YOLO('yolov8n.pt')\n",
    "results=model(\"Screenshot 2023-10-09 132316.png\", show=True)\n",
    "\n",
    "cv2.waitKey(0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-09T07:26:02.013122Z",
     "start_time": "2023-10-09T07:25:47.112020600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
