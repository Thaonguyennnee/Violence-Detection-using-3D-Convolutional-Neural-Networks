{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d833dbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from network import C3D_model\n",
    "import cv2\n",
    "import time\n",
    "torch.backends.cudnn.benchmark = True\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cdfcf6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CenterCrop(frame, size):\n",
    "    h, w = np.shape(frame)[0:2]\n",
    "    th, tw = size\n",
    "    x1 = int(round((w - tw) / 2.))\n",
    "    y1 = int(round((h - th) / 2.))\n",
    "\n",
    "    frame = frame[y1:y1 + th, x1:x1 + tw, :]\n",
    "    return np.array(frame).astype(np.uint8)\n",
    "\n",
    "\n",
    "def center_crop(frame):\n",
    "    frame = frame[8:120, 30:142, :]\n",
    "    return np.array(frame).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fe71a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device being used: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "C3D(\n",
       "  (conv1): Conv3d(3, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (pool1): MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (pool2): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3a): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (conv3b): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (pool3): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv4a): Conv3d(256, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (conv4b): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (pool4): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv5a): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (conv5b): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "  (pool5): MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding=(0, 1, 1), dilation=1, ceil_mode=False)\n",
       "  (fc6): Linear(in_features=8192, out_features=4096, bias=True)\n",
       "  (fc7): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  (fc8): Linear(in_features=4096, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device being used:\", device)\n",
    "\n",
    "with open('./dataloaders/2class.txt', 'r') as f:\n",
    "    class_names = f.readlines()\n",
    "#    f.close()\n",
    "# init model\n",
    "model = C3D_model.C3D(num_classes=2)\n",
    "checkpoint = torch.load('/home/seino_tuanpn7/ThaoNguyen/video-recognition/C3D-traindata_epoch-19.pth.tar', map_location=lambda storage, loc: storage)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2320f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "violence\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-9c79f2781496>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ThaoNguyen/video-recognition/network/C3D_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8192\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc6\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "pathvideos = '/home/seino_tuanpn7/violence/data/violence-data'\n",
    "pathtest = '/home/seino_tuanpn7/ThaoNguyen/video-recognition/traindata/train'\n",
    "pathval = '/home/seino_tuanpn7/ThaoNguyen/video-recognition/traindata/val'\n",
    "video_extensions = ['.mp4', '.avi', '.mpg', '.mov']\n",
    "y_true = []\n",
    "y_pred = []\n",
    "foldername = os.listdir(pathtest)\n",
    "for n in foldername:\n",
    "    filename = os.listdir(pathtest + '/' + n)\n",
    "    print(n)\n",
    "    for name in filename:\n",
    "        for ext in video_extensions:\n",
    "            video = pathvideos + '/' + n + '/' + name + ext\n",
    "            cap = cv2.VideoCapture(video)\n",
    "            if cap.isOpened():\n",
    "                video_paths = video\n",
    "                y_true.append(n)\n",
    "#                 print(video)\n",
    "        cap = cv2.VideoCapture(video_paths)\n",
    "        retaining = True\n",
    "\n",
    "        clip = []\n",
    "\n",
    "        while retaining:\n",
    "            start = time.time()\n",
    "            retaining, frame = cap.read()\n",
    "            if not retaining and frame is None:\n",
    "                continue\n",
    "            tmp_ = center_crop(cv2.resize(frame, (171, 128)))\n",
    "            tmp = tmp_ - np.array([[[90.0, 98.0, 102.0]]])\n",
    "            clip.append(tmp)\n",
    "            if len(clip) == 16:\n",
    "                inputs = np.array(clip).astype(np.float32)\n",
    "                inputs = np.expand_dims(inputs, axis=0)\n",
    "                inputs = np.transpose(inputs, (0, 4, 1, 2, 3))\n",
    "                inputs = torch.from_numpy(inputs)\n",
    "                inputs = torch.autograd.Variable(inputs, requires_grad=False).to(device)\n",
    "                with torch.no_grad():\n",
    "                    outputs = model.forward(inputs)\n",
    "\n",
    "                probs = torch.nn.Softmax(dim=1)(outputs)\n",
    "                label = torch.max(probs, 1)[1].detach().cpu().numpy()[0]\n",
    "\n",
    "                cv2.putText(frame, class_names[label].split(' ')[-1].strip(), (20, 20),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6,\n",
    "                            (0, 0, 255), 1)\n",
    "                cv2.putText(frame, \"prob: %.4f\" % probs[0][label], (20, 40),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6,\n",
    "                            (0, 0, 255), 1)\n",
    "                clip.pop(0)\n",
    "\n",
    "            cv2.imshow('result', frame)\n",
    "            cv2.waitKey(5)\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "245b2e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "y_true= [1,1,0,0,1]\n",
    "y_pred= [1,1,1,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52b3a447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.7499999999999999\n",
      "1.0\n",
      "0.6\n"
     ]
    }
   ],
   "source": [
    "print(len(y_true)-len(y_pred))\n",
    "print(f1_score(y_true,y_pred))\n",
    "print(recall_score(y_true,y_pred))\n",
    "print(precision_score(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f1d59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.f1_score(y_true, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
